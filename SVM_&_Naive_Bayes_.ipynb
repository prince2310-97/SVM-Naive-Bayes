{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "- A Support Vector Machine (SVM) works by finding the optimal boundary to separate different classes of data.\n",
        "\n",
        "How it works:\n",
        "\n",
        "- It tries to find a decision boundary (a line or hyperplane) that has the maximum possible margin from the nearest data points of any class.\n",
        "\n",
        "- These closest data points are called support vectors—they are the only ones that define the position of the boundary.\n",
        "\n",
        "- For data that isn't linearly separable, it uses the \"kernel trick\" to project the data into a higher dimension where finding a clear separating boundary becomes possible."
      ],
      "metadata": {
        "id": "LTOQ2Y9xTSpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "-:- Hard Margin SVM:\n",
        "\n",
        "- Aims to find a hyperplane that perfectly separates two classes with no misclassifications.\n",
        "- Assumes data is linearly separable without noise or outliers.\n",
        "- Maximizes the margin (distance between hyperplane and nearest data points, called support vectors).\n",
        "- Inflexible: fails if data is not perfectly separable.\n",
        "- Risk of overfitting in noisy datasets.\n",
        "\n",
        "-:- Soft Margin SVM:\n",
        "\n",
        "- Allows some misclassifications to achieve better generalization, especially for noisy or non-linearly separable data.\n",
        "- Introduces a slack variable to permit data points to be on the wrong side of the margin or hyperplane.\n",
        "- Balances margin maximization with a penalty for misclassifications (controlled by parameter C).\n",
        "- More robust: works with real-world, noisy, or overlapping data.\n",
        "- Lower C emphasizes larger margin; higher C emphasizes fewer misclassifications."
      ],
      "metadata": {
        "id": "KE0BeCSxVP9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "- The Kernel Trick is a mathematical technique that allows an SVM to find a nonlinear decision boundary without explicitly transforming the data into a higher-dimensional space, which would be computationally expensive.\n",
        "\n",
        " It works by computing the similarity (dot product) between pairs of data points in the original feature space using a kernel function. This lets the SVM operate in a high-dimensional space while all calculations remain efficient in the original space.\n",
        "\n",
        "Example Kernel: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "- What it does: The RBF kernel measures how close points are to each other. It can create complex, non-linear boundaries.\n",
        "\n",
        "- Use Case: It is the most common general-purpose kernel. Use it when there is no obvious linear separation between classes, like for complex, overlapping datasets (e.g., medical image classification or handwritten digit recognition). It's a good default choice when you're unsure which kernel to use."
      ],
      "metadata": {
        "id": "qPtpsq03WpJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "- A Naïve Bayes classifier is a simple probabilistic algorithm based on applying Bayes’ Theorem with a strong assumption: it assumes that every feature used to predict the class is independent of all the others, given the class.\n",
        "\n",
        "- It's called \"naïve\" because this assumption of feature independence is very rarely true in real-world data. For example, in classifying an email as spam, the presence of the word \"win\" and the word \"prize\" are not independent—they often appear together. The algorithm \"naively\" assumes they are unrelated to simplify the calculation."
      ],
      "metadata": {
        "id": "cuBnqwS6XLwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        " 1. Gaussian Naïve Bayes\n",
        "\n",
        "Description: Assumes that continuous numerical features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Use Case: Use this when your features are continuous values (e.g., height, weight, temperature, income). It's common for general numerical data.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Description: Designed for discrete counts, especially frequency counts. It uses the frequency of events (like word counts) for classification.\n",
        "\n",
        "Use Case: Ideal for text classification where features are word counts or TF-IDF scores (e.g., document categorization, spam filtering).\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Description: Designed for binary/boolean features. It models features that are either 1 (present) or 0 (absent).\n",
        "\n",
        "Use Case: Use for text classification where features indicate only the presence or absence of a word (ignoring frequency), or for any dataset with on/off features.\n",
        "\n"
      ],
      "metadata": {
        "id": "fe_tj-DUXseI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "# ● Load the Iris dataset\n",
        "# ● Train an SVM Classifier with a linear kernel\n",
        "# ● Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Number of support vectors for each class:\", svm_classifier.n_support_)\n",
        "print(\"Total number of support vectors:\", len(svm_classifier.support_vectors_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCRwMERxXKuO",
        "outputId": "72fe3e55-4743-427d-aa50-3f80dd4ede2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Number of support vectors for each class: [ 3 11 10]\n",
            "Total number of support vectors: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset\n",
        "# ● Train a Gaussian Naïve Bayes model\n",
        "# ● Print its classification report including precision, recall, and F1-score.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY0nqA5eVKt1",
        "outputId": "531fbbe8-3862-403f-c58d-2ac315c60085"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "# C and gamma.\n",
        "# ● Print the best hyperparameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "best_accuracy = grid.best_score_\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Cross-validation Accuracy:\", best_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q58eyUxaeBK",
        "outputId": "87e9e4d9-18d0-4798-f6d1-23e9a2081b23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Cross-validation Accuracy: 0.6946666666666667\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "# sklearn.datasets.fetch_20newsgroups).\n",
        "# ● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "categories = ['sci.space', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test_tfidf)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_test_bin = lb.fit_transform(y_test)\n",
        "\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred_proba[:, 1])\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-b6eHh6bQMI",
        "outputId": "1e495b48-d981-46c2-baac-15b286c8d9ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9961588300629396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "# email communications.\n",
        "# Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "# contain:\n",
        "# ● Text with diverse vocabulary\n",
        "# ● Potential class imbalance (far more legitimate emails than spam)\n",
        "# ● Some incomplete or missing data\n",
        "# Explain the approach you would take to:\n",
        "# ● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "# ● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "# ● Address class imbalance\n",
        "# ● Evaluate the performance of your solution with suitable metrics\n",
        "# And explain the business impact of your solution.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        'Win a free iPhone', 'Meeting at 10am', 'Claim your prize now',\n",
        "        'Lunch at office', 'Get free lottery ticket', 'Project deadline tomorrow',\n",
        "        'Congratulations! You won', 'Dinner at 8pm'\n",
        "    ],\n",
        "    'label': ['spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing values\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# Split data (50% so both classes appear in test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.5, random_state=42, stratify=df['label'])\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_test_vec = tfidf.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score((y_test=='spam').astype(int), (y_pred=='spam').astype(int)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSIOvMLmbh1F",
        "outputId": "7a26185a-2f8f-4abd-a8ff-671b383d9791"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.50      1.00      0.67         2\n",
            "        spam       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.50         4\n",
            "   macro avg       0.25      0.50      0.33         4\n",
            "weighted avg       0.25      0.50      0.33         4\n",
            "\n",
            "ROC-AUC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.10 ANSWER\n",
        "\n",
        "1. Preprocessing the Data\n",
        "\n",
        "Text Cleaning: Remove punctuation, lowercase, stopwords, and perform stemming/lemmatization.\n",
        "\n",
        "Vectorization: Use TF-IDF (better than Bag of Words for diverse vocabulary).\n",
        "\n",
        "Handle Missing Data: Fill missing email text with empty string or drop rows if labels are missing.\n",
        "\n",
        "2. Choosing Model: SVM vs Naïve Bayes\n",
        "\n",
        "Naïve Bayes: Works well with text because of word independence assumption + fast training.\n",
        "\n",
        "SVM: Handles high-dimensional data but slower and needs tuning.\n",
        "✔ Here, Multinomial Naïve Bayes is ideal for text classification.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Use SMOTE (Synthetic Minority Oversampling) or class weights.\n",
        "\n",
        "Alternatively, undersample majority or oversample minority.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Accuracy is misleading → use:\n",
        "\n",
        "Precision & Recall (Spam detection needs high recall)\n",
        "\n",
        "F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Reduce spam → better user experience, security\n",
        "\n",
        "Improved productivity (less manual filtering)\n",
        "\n",
        "Maintain brand trust (avoid phishing/spam reaching users)"
      ],
      "metadata": {
        "id": "T0jIoO7Uckh6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfSlvfn9c2WV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}